{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .jp-CodeCell {\n",
    "        overflow-x: auto;\n",
    "    }\n",
    "    .jp-CodeCell pre {\n",
    "        white-space: pre-wrap !important;\n",
    "        word-break: break-all !important;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating a Pattern Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pattern_point(label=None):\n",
    "    if label == None:\n",
    "        r = math.sqrt(random.random())*2\n",
    "        theta = random.random()*math.pi+math.pi/2\n",
    "        if random.random() >= 0.5:\n",
    "            theta += math.pi\n",
    "            x1 = r*math.cos(theta)\n",
    "            x2 = r*math.sin(theta) - 1\n",
    "        else:\n",
    "            x1 = r*math.cos(theta)\n",
    "            x2 = r*math.sin(theta)\n",
    "#        x1 = -2*random.random()\n",
    "#        x2 = random.random()*math.sqrt((2-x1)*(2+x1))*2-math.sqrt((2-x1)*(2+x1))\n",
    "        return np.array([x1, x2])\n",
    "    else:\n",
    "        print(\"problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_pattern_point(x1, x2):\n",
    "    if x1 < 0:\n",
    "        if x1*x1+x2*x2 > 1.0:\n",
    "            return \"C1\"\n",
    "        elif x1*x1+x2*x2 < 1.0:\n",
    "            return \"C2\"\n",
    "        else:\n",
    "            return \"C1\" if random.random() < 0.5 else \"C2\"\n",
    "    elif x1 > 0:\n",
    "        if x1*x1+(x2+1)*(x2+1) > 1.0:\n",
    "            return \"C2\"\n",
    "        elif x1*x1+(x2+1)*(x2+1) < 1.0:\n",
    "            return \"C1\"\n",
    "        else:\n",
    "            return \"C1\" if random.random() < 0.5 else \"C2\"\n",
    "    else:\n",
    "        if x2 < -2 or (x2 < 1 and x2 > 0):\n",
    "            return \"C2\"\n",
    "        elif x2 > 1 or (x2 > -2 and x2 < -1):\n",
    "            return \"C1\"\n",
    "        else:\n",
    "            return \"C1\" if random.random() < 0.5 else \"C2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pattern_points(n, label=None):\n",
    "    if label == None:\n",
    "        return np.array([generate_pattern_point(None) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_pattern_points(pattern_point_list):\n",
    "    return np.array([labelize_pattern_point(x1, x2) for (x1, x2) in pattern_point_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pattern_points(pattern_point_list, label_list):\n",
    "    points_c1 = [pattern_point_list[i] for i in range(len(pattern_point_list)) if label_list[i] == 'C1']\n",
    "    points_c2 = [pattern_point_list[i] for i in range(len(pattern_point_list)) if label_list[i] == 'C2']\n",
    "    x_c1, y_c1 = zip(*points_c1)\n",
    "    x_c2, y_c2 = zip(*points_c2)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_c1, y_c1, color='red', s=1, label='C1')\n",
    "    plt.scatter(x_c2, y_c2, color='blue', s=1, label='C2')\n",
    "    plt.title(str(len(pattern_point_list))+' Pattern Points')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_pattern_points(150000, None)\n",
    "y_label = labelize_pattern_points(X)\n",
    "X = X.T\n",
    "y_label = y_label.reshape(1, len(y_label))\n",
    "print(X.shape)\n",
    "print(y_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pattern_points(X.T, y_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used for binary classification\n",
    "def labels_to_one_hot(y_labels, classes):\n",
    "    n_samples = len(y_labels)\n",
    "    n_classes = len(classes)\n",
    "    y_one_hot = np.zeros((n_samples, n_classes))\n",
    "    for i, label in enumerate(y_labels):\n",
    "        index = classes.index(label)\n",
    "        y_one_hot[i, index] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for binary classification\n",
    "def labels_to_float(y_labels, positive_class):\n",
    "    return np.array([1 if label == positive_class else 0 for label in y_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:, :int(len(X[0])*0.7)]\n",
    "X_test = X[:, int(len(X[0])*0.7):]\n",
    "y = np.array([labels_to_float(y_label, 'C1')])\n",
    "y_train = y[:, :int(y.shape[1]*0.7)]\n",
    "y_test = y[:, int(y.shape[1]*0.7):]\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pattern_points(X, X_train):\n",
    "    mean = np.mean(X_train, axis=1).reshape(1, -1).T    \n",
    "    std_dev = np.std(X_train, axis=1).reshape(1, -1).T\n",
    "    X = (X - mean) / std_dev\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_pattern_points(X_train, X_train)\n",
    "X_test = normalize_pattern_points(X_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -709, 709)\n",
    "#    z = np.clip(z, -1000, 1000)\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
    "    return e_z / np.sum(e_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    A = sigmoid(z)\n",
    "    return A * (1 - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z):\n",
    "    grad = np.zeros(z.shape)\n",
    "    grad[z > 0] = 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOES NOT WORK TODO\n",
    "def softmax_derivative(z):\n",
    "    s = softmax(z)\n",
    "    return np.diagflat(s) - np.outer(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test = np.array([-1, 0, 1])\n",
    "print(\"Sigmoid derivative:\", sigmoid_derivative(Z_test))\n",
    "print(\"ReLU derivative:\", relu_derivative(Z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_normal_distribution(layers_dim, mean, std_dev):\n",
    "    n = len(layers_dim)\n",
    "\n",
    "    b_list = [0]*(n)\n",
    "    w_list = [0]*(n)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w_list[i] = np.random.randn(layers_dim[i], layers_dim[i-1]) * std_dev + mean\n",
    "        b_list[i] = np.zeros((layers_dim[i], 1))\n",
    "    return w_list, b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list, b_list = init_weights_normal_distribution([2, 2, 1], 0, 0.1)\n",
    "print(\"w_list = {}\".format(w_list))\n",
    "print(\"b_list = {}\".format(b_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w_list, b_list):\n",
    "    n_layers = len(w_list)\n",
    "    A = X\n",
    "    z_list = [0]*(n_layers)\n",
    "    a_list = [X]*(n_layers)  # Initialize with input layer\n",
    "\n",
    "    for i in range(1, n_layers):\n",
    "        Z = np.dot(w_list[i], A) + b_list[i]\n",
    "        A = relu(Z) if i < n_layers - 1 else sigmoid(Z)\n",
    "        z_list[i] = Z\n",
    "        a_list[i] = A\n",
    "    return z_list, a_list, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([1., 1.]).reshape(2, 1)\n",
    "print(\"X = {}\".format(X1))\n",
    "z_list, a_list, A = forward(X1, w_list, b_list)\n",
    "print(z_list)\n",
    "print(a_list)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(A, y, epsilon=1e-8):\n",
    "    assert A.shape == y.shape\n",
    "    m = A.shape[1]\n",
    "    loss = -1/m * (np.dot(y, np.log(A.T + epsilon)) + np.dot(1-y, np.log((1 - A).T + epsilon)))\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.9, 0.3]])\n",
    "y1 = np.array([[1, 0]])\n",
    "\n",
    "loss = cross_entropy_loss(A, y1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w_list, b_list, z_list, a_list, X, y, epsilon=1e-8):\n",
    "    n_layers = len(w_list)\n",
    "    m = y.shape[0]\n",
    "    dw_list = [0]*n_layers\n",
    "    db_list = [0]*n_layers\n",
    "    a_list[0] = X\n",
    "\n",
    "    for i in range(n_layers-1, 0, -1):\n",
    "        A, A_prev, Z = a_list[i], a_list[i-1], z_list[i]\n",
    "        W = w_list[i]\n",
    "        if i == n_layers-1:\n",
    "            dA = -np.divide(y, A + epsilon) + np.divide(1 - y, 1 - A + epsilon)\n",
    "        if i == n_layers-1:\n",
    "            dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        dw_list[i] = dW\n",
    "        db_list[i] = db\n",
    "\n",
    "    return dw_list, db_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_list, db_list = backward(w_list, b_list, z_list, a_list, np.array([[1], [1]]), np.array([[1]]))\n",
    "print(dw_list)\n",
    "print(db_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w_list, b_list, dw_list, db_list, learning_rate):\n",
    "    n_layers = len(w_list)\n",
    "    for i in range(n_layers):\n",
    "        if dw_list[i] is not None and db_list[i] is not None:\n",
    "            w_list[i] -= learning_rate * dw_list[i] #TODO: adaptative learning rate\n",
    "            b_list[i] -= learning_rate * db_list[i]\n",
    "    return w_list, b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_adam(w_list, b_list, dw_list, db_list, learning_rate, t, m_w_list, v_w_list, m_b_list, v_b_list, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    n_layers = len(w_list)\n",
    "    for i in range(n_layers):\n",
    "        if dw_list[i] is not None and db_list[i] is not None:\n",
    "            # Update biased first moment estimate for weights and biases\n",
    "            m_w_list[i] = beta1 * m_w_list[i] + (1 - beta1) * dw_list[i]\n",
    "            m_b_list[i] = beta1 * m_b_list[i] + (1 - beta1) * db_list[i]\n",
    "\n",
    "            # Update biased second raw moment estimate for weights and biases\n",
    "            v_w_list[i] = beta2 * v_w_list[i] + (1 - beta2) * (dw_list[i] ** 2)\n",
    "            v_b_list[i] = beta2 * v_b_list[i] + (1 - beta2) * (db_list[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate for weights and biases\n",
    "            m_w_hat = m_w_list[i] / (1 - beta1 ** t)\n",
    "            m_b_hat = m_b_list[i] / (1 - beta1 ** t)\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate for weights and biases\n",
    "            v_w_hat = v_w_list[i] / (1 - beta2 ** t)\n",
    "            v_b_hat = v_b_list[i] / (1 - beta2 ** t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            w_list[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "            b_list[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "    return w_list, b_list, m_w_list, v_w_list, m_b_list, v_b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, batch_size):\n",
    "    n = X.shape[1]\n",
    "    batches = [range(i, min(X.shape[1], i+batch_size)) for i in range(0, n, batch_size)]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    assert y.shape[0] == 1\n",
    "    assert y.shape == y_pred.shape\n",
    "    y_pred = np.round(y_pred)\n",
    "    return float(np.dot(y, y_pred.T) + np.dot(1-y, 1-y_pred.T)) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop conditions: max_epochs and acc_stop\n",
    "def train(X_train, y_train, layers, batch_size=100, max_epochs=100, learning_rate=0.1, acc_stop=0.99, loss_diff=0.01, validation_split=0.3, mean=0, std_dev=0.01, optimizer='Adam', verbose=True):\n",
    "    X_validation = X_train[:, :int(validation_split*len(X_train[0]))]\n",
    "    X_train = X_train[:, int(validation_split*len(X_train[0])):]\n",
    "    y_validation = y_train[:, :int(validation_split*len(y_train[0]))]\n",
    "    y_train = y_train[:, int(validation_split*len(y_train[0])):]\n",
    "\n",
    "    train_loss_list = [0]*max_epochs\n",
    "    validation_loss_list = [0]*max_epochs\n",
    "    train_accuracy_list = []\n",
    "    validation_accuracy_list = [0]*max_epochs\n",
    "    \n",
    "    m_w_list, v_w_list, m_b_list, v_b_list = [0]*len(layers), [0]*len(layers), [0]*len(layers), [0]*len(layers)\n",
    "    # prepare batch training\n",
    "    batches = generate_batch(X_train, batch_size)\n",
    "    # init weights\n",
    "    w_list, b_list = init_weights_normal_distribution(layers, mean, std_dev)\n",
    "\n",
    "    epoch = 1\n",
    "    while epoch <= max_epochs:\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for batch in batches:\n",
    "            X = X_train[:, batch]\n",
    "            y = y_train[:, batch]\n",
    "            z_list, a_list, A = forward(X, w_list, b_list)\n",
    "            dw_list, db_list = backward(w_list, b_list, z_list, a_list, X, y)\n",
    "            if optimizer == 'Adam':\n",
    "                w_list, b_list, m_w_list, v_w_list, m_b_list, v_b_list = optimize_with_adam(w_list, b_list, dw_list, db_list, learning_rate, epoch, m_w_list, v_w_list, m_b_list, v_b_list, beta1=0.9, beta2=0.999, epsilon=1e-10) #beta1=0.9 beta0.999\n",
    "            else:\n",
    "                w_list, b_list = optimize(w_list, b_list, dw_list, db_list, learning_rate)\n",
    "\n",
    "            avg_loss += cross_entropy_loss(A, y)\n",
    "\n",
    "        #Train loss\n",
    "        train_loss_list[epoch-1] = avg_loss / len(batches) #loss calculation\n",
    "\n",
    "        #Validation loss\n",
    "        _, _, valid_A = forward(X_validation, w_list, b_list)\n",
    "        validation_loss_list[epoch-1] = cross_entropy_loss(valid_A, y_validation)\n",
    "\n",
    "        #Train accuracy\n",
    "        _, _, A = forward(X_train, w_list, b_list)\n",
    "        train_accuracy = accuracy(y_train, A)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        #Validation accuracy\n",
    "        _, _, A = forward(X_validation, w_list, b_list)\n",
    "        validation_accuracy_list[epoch-1] = accuracy(y_validation, A)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch}/{max_epochs}, Loss: {train_loss_list[epoch-1]}, Accuracy: {train_accuracy}\", end='\\r')\n",
    "\n",
    "        if validation_accuracy_list[epoch-1] > acc_stop and abs(validation_loss_list[epoch-1] - train_loss_list[epoch-1]) < loss_diff:\n",
    "            break\n",
    "        epoch += 1\n",
    "\n",
    "    epoch = min(epoch, max_epochs)\n",
    "    return w_list, b_list, epoch, train_loss_list[:epoch], validation_loss_list[:epoch], train_accuracy_list, validation_accuracy_list[:epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "w_list, b_list, nb_epochs, train_loss_list, validation_loss_list, train_accuracy_list, validation_accuracy_list = train(X_train, y_train, [2, 50, 25, 1], batch_size=500, max_epochs=max_epochs, learning_rate=0.005, acc_stop = 0.999, loss_diff=0.002, validation_split = 0.2, mean=0, std_dev=0.05, optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "w_list, b_list, nb_epochs, train_loss_list, validation_loss_list, train_accuracy_list, validation_accuracy_list = train(X_train, y_train, [2, 50, 25, 1], batch_size=1500000, max_epochs=max_epochs, learning_rate=0.001, acc_stop = 0.99, loss_diff=0.002, validation_split = 0.2, mean=0, std_dev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "w_list, b_list, nb_epochs, train_loss_list, validation_loss_list, train_accuracy_list, validation_accuracy_list = train(X_train, y_train, [2, 50, 25, 1], 500, max_epochs, learning_rate=0.01, acc_stop = 0.99, loss_diff=0.002, validation_split = 0.2, mean=0, std_dev=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "n_avg = 10\n",
    "#hyperparameters = [1, 10, 100, 200, 500, 750, 900, 1000, 1100, 1200, 1300, 1400, 1500, 2000, 2500, 5000, 10000, len(X_train[0])] #batch size\n",
    "#hyperparameters = [5e-6, 1e-5, 5e-5, 0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.05] #learning rate\n",
    "hyperparameters = [0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "#hyperparameters = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20, 30, 50, 100, 150] #number of neurons\n",
    "nb_epochs, train_loss_list, validation_loss_list, train_accuracy_list, validation_accuracy_list, test_accuracy_list = [0.]*len(hyperparameters), [0.]*len(hyperparameters), [0.]*len(hyperparameters), [0.]*len(hyperparameters), [0.]*len(hyperparameters), [0.]*len(hyperparameters)\n",
    "avg_train_accuracy, avg_validation_accuracy, avg_test_accuracy = [0.]*len(hyperparameters), [0.]*len(hyperparameters), [0.]*len(hyperparameters)\n",
    "avg_nb_epochs = [0]*len(hyperparameters)\n",
    "for k in range(n_avg):\n",
    "    print(k)\n",
    "    for i in range(len(hyperparameters)):\n",
    "        w_list, b_list, nb_epochs[i], train_loss_list[i], validation_loss_list[i], train_accuracy_list[i], validation_accuracy_list[i] = train(X_train, y_train, [2, 50, 25, 1], batch_size=500, max_epochs=max_epochs, learning_rate=hyperparameters[i], acc_stop = 0.97, loss_diff=0.2, validation_split=0.2, mean=0, std_dev=0.05, optimizer='standard', verbose=False)\n",
    "        z_list, a_list, pred = forward(X_test, w_list, b_list)\n",
    "        test_accuracy_list[i] = accuracy(y_test, pred)\n",
    "        avg_train_accuracy[i] += train_accuracy_list[i][-1]\n",
    "        avg_validation_accuracy[i] += validation_accuracy_list[i][-1]\n",
    "        avg_test_accuracy[i] += test_accuracy_list[i]        \n",
    "        avg_nb_epochs[i] += nb_epochs[i]\n",
    "        print(train_accuracy_list[i][-1], validation_accuracy_list[i][-1], test_accuracy_list[i])\n",
    "\n",
    "avg_train_accuracy = list(map(lambda x: x/n_avg, avg_train_accuracy))\n",
    "avg_validation_accuracy = list(map(lambda x: x/n_avg, avg_validation_accuracy))\n",
    "avg_test_accuracy = list(map(lambda x: x/n_avg, avg_test_accuracy))\n",
    "avg_nb_epochs = list(map(lambda x: x/n_avg, avg_nb_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(avg_train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracies vs batch size\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(hyperparameters, avg_train_accuracy, marker='x')\n",
    "plt.plot(hyperparameters, avg_validation_accuracy, marker='o')\n",
    "plt.plot(hyperparameters, avg_test_accuracy, marker='^')\n",
    "plt.xscale('log')\n",
    "plt.xticks(hyperparameters, labels=[str(x) for x in hyperparameters])\n",
    "plt.title('Accuracy vs Learning rate')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation', 'Test'], loc='upper right')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot epoch for convergence\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(hyperparameters, avg_nb_epochs, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xticks(hyperparameters, labels=[str(x) for x in hyperparameters])\n",
    "plt.title('Convergence time vs Learning rate')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Number of epochs')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1, nb_epochs + 1), train_loss_list)\n",
    "plt.plot(np.arange(1, nb_epochs + 1), validation_loss_list)\n",
    "plt.title('Loss vs epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(1, nb_epochs + 1), train_accuracy_list)\n",
    "plt.plot(np.arange(1, nb_epochs + 1), validation_accuracy_list)\n",
    "plt.title('Accuracy vs epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list, a_list, pred = forward(X_test, w_list, b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy(y_test, pred)\n",
    "print(f'accuracy: {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(X_test, pred):\n",
    "    pattern_points = X_test.T\n",
    "    points_c1 = [pattern_points[i] for i in range(len(pattern_points)) if pred[0][i] > 0.5]\n",
    "    points_c2 = [pattern_points[i] for i in range(len(pattern_points)) if pred[0][i] <= 0.5]\n",
    "    if points_c1 != []:\n",
    "        x_c1, y_c1 = zip(*points_c1)\n",
    "    if points_c2 != []:\n",
    "        x_c2, y_c2 = zip(*points_c2)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    if points_c1 != []:\n",
    "        plt.scatter(x_c1, y_c1, color='blue', s=1, label='C1', marker='o')\n",
    "    if points_c2 != []:\n",
    "        plt.scatter(x_c2, y_c2, color='red', s=1, label='C2')\n",
    "   \n",
    "    plt.title('Pattern Points')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.text(-2.9, -2.9, \"accuracy: \"+str(test_accuracy), size=12, color=(0.7, 0, 0, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(X_test, pred):\n",
    "    pattern_points = X_test.T\n",
    "    x, y = zip(*pattern_points)\n",
    "\n",
    "#    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x, y, s=0.25, c=pred[0], cmap=plt.cm.get_cmap(\"seismic\"))\n",
    "    plt.colorbar()       \n",
    "    plt.title('Pattern Points')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.text(-2.9, -2.9, \"accuracy: \"+str(test_accuracy), size=12, color=(0.7, 0, 0, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(X_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y, y_pred):\n",
    "    assert y.shape[0] == 1\n",
    "    assert y.shape == y_pred.shape\n",
    "    \n",
    "    # Round predictions to 0 or 1\n",
    "    y_pred_rounded = np.round(y_pred)\n",
    "    \n",
    "    # Calculate true positives, true negatives, false positives, and false negatives\n",
    "    TP = np.sum((y == 1) & (y_pred_rounded == 1))\n",
    "    TN = np.sum((y == 0) & (y_pred_rounded == 0))\n",
    "    FP = np.sum((y == 0) & (y_pred_rounded == 1))\n",
    "    FN = np.sum((y == 1) & (y_pred_rounded == 0))\n",
    "\n",
    "    conf_matrix = np.array([[TP, FP],\n",
    "                            [FN, TN]])\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues, norm=matplotlib.colors.LogNorm())\n",
    "    plt.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + ['Predicted C1', 'Predicted C2'])\n",
    "    ax.set_yticklabels([''] + ['Actual C1', 'Actual C2'])\n",
    "\n",
    "    for (i, j), val in np.ndenumerate(conf_matrix):\n",
    "        ax.text(j, i, f'{val}', ha='center', va='center')\n",
    "\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
